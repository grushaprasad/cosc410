{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152a0961-5ecb-471d-a380-2f4cadaa676e",
   "metadata": {},
   "source": [
    "# Lab 8: Finetuning \n",
    "\n",
    "### COSC 410B: Spring 2025, Colgate University\n",
    "\n",
    "In this lab you will finetune a pretrained language model, DistilBERT, on Natural Language Inference. \n",
    "\n",
    "#### Goals for this lab:\n",
    "\n",
    "* Work with huggingface datasets and format it in appropriate ways given a task\n",
    "* Work pre-trained language models (load, finetune, evaluate)\n",
    "\n",
    "Note, you will only train the model on a small set of examples because the goal is to help you figure out how to set up a finetuning pipeline for your final projects, not to train a good model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd83d4b1-5dbb-4b90-b139-659790ac2be8",
   "metadata": {},
   "source": [
    "## Part 1: Understand the task\n",
    "\n",
    "You will be working with the Natural Language Inference task using the [MNLI dataset](https://huggingface.co/datasets/SetFit/mnli)\n",
    "\n",
    "Briefly describe: \n",
    "* What is the input to this task?\n",
    "* What is the output?\n",
    "* Do you need to preprocess the input in some way before you can pass it into the model?\n",
    "* What data and task was [DistilBERT](https://arxiv.org/pdf/1910.01108) pre-trained on? \n",
    "* What does it mean to finetune this model on NLI? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b468645f-42bc-4702-8f95-5fd582823e7b",
   "metadata": {},
   "source": [
    "**WRITE YOUR ANSWERS HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c297c-d656-4e01-a85a-c4fc18a23494",
   "metadata": {},
   "source": [
    "## Part 2: Load and preprocess the data\n",
    "\n",
    "In order to train the model, you need to get the data in the following format. \n",
    "```\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['id', 'text', 'label'],\n",
    "        num_rows: 100\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['id', 'text', 'label'],\n",
    "        num_rows: 100\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "In this part load the [MNLI dataset](https://huggingface.co/datasets/SetFit/mnli). Take only the first 100 examples from train and test for the purposes of this lab (because it will take extremely long to train the model otherwise). The preprocess it such that it can be passed into the model! A helper function is provided, and the model name is pre-specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3e738b-8eee-4fdb-9fcb-67804aada0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(dataset, modelname):\n",
    "    \"\"\"Tokenize text data\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    return dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71745cab-9ad9-497e-884e-cb81a537fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a85e73-1bc8-45d4-931b-14b03464c961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45790012-fd14-44f4-9cb3-0b13185aeec7",
   "metadata": {},
   "source": [
    "## Part 3: Evaluate model before finetuning\n",
    "\n",
    "In this part you will load the model and evaluate it before finetuning. You will use the [AutoModelForSequenceClassification](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification). A helper function is provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51cf5e1-d735-4ed7-b5dc-efbc0d563bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data):\n",
    "    \"\"\" Get predictions on data for a classification model m returning predictions and true labels\"\"\"\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    predictions = model(torch.tensor(data['input_ids']), torch.tensor(data['attention_mask']))\n",
    "    predictions = torch.argmax(predictions.logits, dim=-1)\n",
    "    return predictions, torch.tensor(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1c4f9-f273-4ed5-b817-f600d2b64417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "385389ef-04da-4028-be1b-49fd4da8f2c7",
   "metadata": {},
   "source": [
    "## Part 4: Finetune the model \n",
    "\n",
    "Finetune the model and then evaluate the model after finetuning. A helper function is given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ed8d9b-cbf7-4a54-b93c-67259055de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, num_epochs):\n",
    "    \"\"\" Train text model for classification \"\"\" \n",
    "    training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                               num_train_epochs=num_epochs)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data['train'],\n",
    "        eval_dataset=train_data['test']\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b9762-8dcf-475c-a80d-f1613c0ecf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "639a6669-9820-4017-a0a3-d838e3683f42",
   "metadata": {},
   "source": [
    "## Part 5: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a24b12-6231-43bc-a496-0ae3329a5e2c",
   "metadata": {},
   "source": [
    "Answer the following questions\n",
    "\n",
    "* What is being initialized when you load the model?\n",
    "* What weights are being learned during finetuning?\n",
    "* Why might pre-training help a model on this task? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a8e71d-c47c-4d87-a0e4-2bdff0ece3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosc410",
   "language": "python",
   "name": "cosc410"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
